from fastapi import FastAPI, UploadFile, File, HTTPException, Query, Form, BackgroundTasks
from fastapi.responses import Response
from fastapi.middleware.cors import CORSMiddleware
from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
from transformers import AutoTokenizer
import tempfile
import os
import zipfile
from io import BytesIO
from pathlib import Path
from typing import Literal, Optional, List, Dict
import logging
import mimetypes
import uuid
from datetime import datetime
import asyncio

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì‘ì—… ìƒíƒœ ì €ì¥ì†Œ (ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” Redisë‚˜ DB ì‚¬ìš© ê¶Œì¥)
job_store: Dict[str, dict] = {}

app = FastAPI(
    title="Docling Document Converter",
    description="Docling HybridChunkerì™€ Contextualizeë¥¼ í™œìš©í•œ ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì„ Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ” API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Docling ì»¨ë²„í„° ë° HybridChunker ì´ˆê¸°í™”
doc_converter = DocumentConverter()

# HuggingFace í† í¬ë‚˜ì´ì € ì„¤ì • (í•œê¸€ ìµœì í™”)
EMBED_MODEL_ID = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # í•œê¸€ ì²˜ë¦¬ 3ë°° íš¨ìœ¨ì !
MAX_TOKENS = 512  # ì²­í¬ í¬ê¸° ì œí•œ

tokenizer = HuggingFaceTokenizer(
    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),
    max_tokens=MAX_TOKENS,
)

# HybridChunker ì´ˆê¸°í™”
chunker = HybridChunker(
    tokenizer=tokenizer,
    merge_peers=True,  # ì¸ì ‘í•œ ì²­í¬ ë³‘í•© ì˜µì…˜
)

# ì§€ì›ë˜ëŠ” íŒŒì¼ í™•ì¥ì (Docling ì§€ì› í˜•ì‹)
SUPPORTED_EXTENSIONS = {
    '.pdf': 'PDF ë¬¸ì„œ',
    '.docx': 'Word ë¬¸ì„œ',
    '.xlsx': 'Excel ìŠ¤í”„ë ˆë“œì‹œíŠ¸',
    '.pptx': 'PowerPoint í”„ë ˆì  í…Œì´ì…˜',
    '.html': 'HTML íŒŒì¼',
    '.htm': 'HTML íŒŒì¼',
    '.md': 'Markdown íŒŒì¼',
    '.txt': 'í…ìŠ¤íŠ¸ íŒŒì¼',
    '.json': 'JSON íŒŒì¼',
    '.xml': 'XML íŒŒì¼',
    '.csv': 'CSV íŒŒì¼',
    '.jpg': 'JPEG ì´ë¯¸ì§€',
    '.jpeg': 'JPEG ì´ë¯¸ì§€',
    '.png': 'PNG ì´ë¯¸ì§€',
    '.gif': 'GIF ì´ë¯¸ì§€',
    '.bmp': 'BMP ì´ë¯¸ì§€',
}

@app.get("/")
async def root():
    """API ìƒíƒœ ë° ì§€ì› íŒŒì¼ í˜•ì‹ í™•ì¸"""
    return {
        "message": "Docling Document Converter API",
        "status": "running",
        "supported_formats": SUPPORTED_EXTENSIONS,
        "chunking": {
            "engine": "HybridChunker",
            "tokenizer": EMBED_MODEL_ID,
            "max_tokens": MAX_TOKENS,
            "contextualize": True
        },
        "endpoints": {
            "convert": "/convert - íŒŒì¼ì„ Markdownìœ¼ë¡œ ë³€í™˜",
            "convert-chunked": "/convert-chunked - íŒŒì¼ì„ ì²­í¬ë³„ë¡œ ë³€í™˜ (ë™ê¸°)",
            "convert-chunked-async": "/convert-chunked-async - íŒŒì¼ì„ ì²­í¬ë³„ë¡œ ë³€í™˜ (ë¹„ë™ê¸°, ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)",
            "convert-multiple": "/convert-multiple - ì—¬ëŸ¬ íŒŒì¼ì„ ì¼ê´„ ë³€í™˜",
            "job-status": "/job/{job_id} - ë¹„ë™ê¸° ì‘ì—… ìƒíƒœ ì¡°íšŒ",
            "jobs": "/jobs - ëª¨ë“  ì‘ì—… ëª©ë¡ ì¡°íšŒ",
            "delete-job": "DELETE /job/{job_id} - ì‘ì—… ì‚­ì œ ë° ì •ë¦¬",
            "health": "/health - ì„œë²„ ìƒíƒœ í™•ì¸"
        },
        "async_workflow": {
            "description": "ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•œ ë¹„ë™ê¸° ì›Œí¬í”Œë¡œìš°",
            "steps": [
                "1. POST /convert-chunked-async - íŒŒì¼ ì—…ë¡œë“œ ë° job_id ìˆ˜ì‹ ",
                "2. GET /job/{job_id} - ì‘ì—… ìƒíƒœ polling (status: queued/processing/completed/failed)",
                "3. statusê°€ 'completed'ë©´ resultì—ì„œ ì²­í¬ ë°ì´í„° íšë“",
                "4. DELETE /job/{job_id} - ì‘ì—… ì •ë¦¬ (ì„ íƒì‚¬í•­)"
            ]
        }
    }

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy", 
        "library": "Docling with HybridChunker", 
        "supported_files": len(SUPPORTED_EXTENSIONS),
        "tokenizer": EMBED_MODEL_ID,
        "max_tokens": MAX_TOKENS,
        "active_jobs": len(job_store)
    }

@app.post("/convert")
async def convert_file(
    file: UploadFile = File(..., description="ë³€í™˜í•  íŒŒì¼"),
    output_filename: Optional[str] = Query(None, description="ì¶œë ¥ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)"),
    include_metadata: bool = Query(False, description="íŒŒì¼ ë©”íƒ€ë°ì´í„° í¬í•¨"),
    use_chunking: bool = Query(False, description="HybridChunkerë¥¼ ì‚¬ìš©í•œ ì²­í‚¹ ì ìš©"),
    contextualize: bool = Query(True, description="ì²­í¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©")
):
    """
    ë‹¨ì¼ íŒŒì¼ì„ Markdownìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    HybridChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì˜ë¯¸ìˆëŠ” ì²­í¬ë¡œ ë¶„í• í•˜ê³ ,
    contextualize ê¸°ëŠ¥ìœ¼ë¡œ ê° ì²­í¬ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°•í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì§€ì› í˜•ì‹:
    - ë¬¸ì„œ: PDF, Word (.docx), PowerPoint (.pptx), Excel (.xlsx)
    - ì›¹: HTML, Markdown, CSV, JSON, XML
    - ì´ë¯¸ì§€: JPG, PNG, GIF, BMP
    - í…ìŠ¤íŠ¸: TXT
    """
    
    # íŒŒì¼ í™•ì¥ì ê²€ì¦
    file_extension = Path(file.filename).suffix.lower()
    if file_extension not in SUPPORTED_EXTENSIONS:
        supported_list = ', '.join(SUPPORTED_EXTENSIONS.keys())
        raise HTTPException(
            status_code=400, 
            detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {supported_list}"
        )
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì²˜ë¦¬
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
            input_file_path = os.path.join(temp_dir, file.filename)
            with open(input_file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            logger.info(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {input_file_path} ({len(content)} bytes)")
            
            # Doclingìœ¼ë¡œ ë³€í™˜
            logger.info("Docling ë¬¸ì„œ ë³€í™˜ ì‹œì‘")
            result = doc_converter.convert(input_file_path)
            doc = result.document
            logger.info("Docling ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ")
            
            # ì²­í‚¹ ì ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬
            if use_chunking:
                logger.info("HybridChunkerë¡œ ì²­í‚¹ ì‹œì‘")
                chunks = list(chunker.chunk(dl_doc=doc))
                logger.info(f"ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                
                # ê° ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ Markdown ìƒì„±
                markdown_parts = []
                for i, chunk in enumerate(chunks):
                    if contextualize:
                        # contextualizeë¡œ ì»¨í…ìŠ¤íŠ¸ ê°•í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
                        enriched_text = chunker.contextualize(chunk=chunk)
                        markdown_parts.append(f"## ì²­í¬ {i+1}\n\n{enriched_text}\n")
                    else:
                        # ê¸°ë³¸ ì²­í¬ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        markdown_parts.append(f"## ì²­í¬ {i+1}\n\n{chunk.text}\n")
                
                final_content = "\n".join(markdown_parts)
            else:
                # ê¸°ë³¸ Markdown ë³€í™˜ (ì²­í‚¹ ì—†ìŒ)
                final_content = doc.export_to_markdown()
            
            # ì¶œë ¥ íŒŒì¼ëª… ì„¤ì •
            if not output_filename:
                output_filename = Path(file.filename).stem
            
            # Markdown íŒŒì¼ë¡œ ì‘ë‹µ
            return Response(
                content=final_content,
                media_type="text/markdown",
                headers={
                    "Content-Disposition": f"attachment; filename={output_filename}.md"
                }
            )
            
        except Exception as e:
            logger.error(f"ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/convert-chunked")
async def convert_file_chunked(
    file: UploadFile = File(..., description="ë³€í™˜í•  íŒŒì¼"),
    include_metadata: bool = Query(False, description="ì²­í¬ ë©”íƒ€ë°ì´í„° í¬í•¨"),
    contextualize: bool = Query(True, description="ì²­í¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©"),
    max_tokens: Optional[int] = Query(None, description="ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 512)")
):
    """
    íŒŒì¼ì„ HybridChunkerë¡œ ì²­í‚¹í•˜ì—¬ JSON í˜•íƒœë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.
    ê° ì²­í¬ì˜ ìƒì„¸ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ ê°•í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    file_extension = Path(file.filename).suffix.lower()
    if file_extension not in SUPPORTED_EXTENSIONS:
        supported_list = ', '.join(SUPPORTED_EXTENSIONS.keys())
        raise HTTPException(
            status_code=400, 
            detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {supported_list}"
        )
    
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # íŒŒì¼ ì €ì¥
            input_file_path = os.path.join(temp_dir, file.filename)
            with open(input_file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            # ì‚¬ìš©ì ì •ì˜ í† í° ìˆ˜ê°€ ìˆë‹¤ë©´ ìƒˆë¡œìš´ chunker ìƒì„±
            if max_tokens and max_tokens != MAX_TOKENS:
                custom_tokenizer = HuggingFaceTokenizer(
                    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),
                    max_tokens=max_tokens,
                )
                custom_chunker = HybridChunker(
                    tokenizer=custom_tokenizer,
                    merge_peers=True,
                )
                active_chunker = custom_chunker
            else:
                active_chunker = chunker
            
            # ë¬¸ì„œ ë³€í™˜
            result = doc_converter.convert(input_file_path)
            doc = result.document
            
            # Excel íŒŒì¼ì¸ ê²½ìš° ì‹œíŠ¸ ì •ë³´ ì§ì ‘ ì¶”ì¶œ
            excel_sheets = []
            if file_extension == '.xlsx':
                try:
                    import openpyxl
                    wb = openpyxl.load_workbook(input_file_path, read_only=True)
                    excel_sheets = wb.sheetnames
                    wb.close()
                    logger.info(f"Excel ì‹œíŠ¸ ê°ì§€: {excel_sheets}")
                except Exception as e:
                    logger.warning(f"Excel ì‹œíŠ¸ ì´ë¦„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # HybridChunkerë¡œ ì²­í‚¹
            chunks = list(active_chunker.chunk(dl_doc=doc))
            
            # ì²­í¬ ì •ë³´ ìˆ˜ì§‘
            chunk_data = []
            for i, chunk in enumerate(chunks):
                chunk_info = {
                    "chunk_id": i + 1,
                    "text": chunk.text,
                    "text_length": len(chunk.text),
                    "token_count": tokenizer.count_tokens(chunk.text) if hasattr(tokenizer, 'count_tokens') else None
                }
                
                # ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©
                if contextualize:
                    enriched_text = active_chunker.contextualize(chunk=chunk)
                    chunk_info["contextualized_text"] = enriched_text
                    chunk_info["contextualized_length"] = len(enriched_text)
                
                # í˜ì´ì§€ ì •ë³´ ë° í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (í•­ìƒ í™œì„±í™”)
                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
                if hasattr(chunk, 'meta'):
                    chunk_info["metadata"] = chunk.meta.export_json_dict() if hasattr(chunk.meta, 'export_json_dict') else str(chunk.meta)
                
                # ë””ë²„ê·¸: ì²­í¬ êµ¬ì¡° í™•ì¸
                chunk_info["debug_chunk_attrs"] = [attr for attr in dir(chunk) if not attr.startswith('_')]
                if hasattr(chunk, 'meta'):
                    chunk_info["debug_meta_attrs"] = [attr for attr in dir(chunk.meta) if not attr.startswith('_')]
                
                # í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ - íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬
                page_numbers = set()
                bbox_data = []
                sheet_names = set()  # Excel ì‹œíŠ¸ ì´ë¦„
                
                # PDF ë“± í˜ì´ì§€ ê¸°ë°˜ ë¬¸ì„œ
                if hasattr(chunk, 'meta') and hasattr(chunk.meta, 'doc_items'):
                    for doc_item in chunk.meta.doc_items:
                        if hasattr(doc_item, 'prov') and isinstance(doc_item.prov, list):
                            for prov in doc_item.prov:
                                # page_noê°€ ìˆìœ¼ë©´ ì‚¬ìš© (PDF)
                                if hasattr(prov, 'page_no') and prov.page_no > 0:
                                    page_numbers.add(prov.page_no)
                                    
                                    bbox_info = {
                                        "page": prov.page_no,
                                        "label": getattr(doc_item, 'label', 'unknown')
                                    }
                                    
                                    # bbox ì •ë³´ ì¶”ê°€
                                    if hasattr(prov, 'bbox'):
                                        bbox_info["bbox"] = {
                                            "l": getattr(prov.bbox, 'l', 0),
                                            "t": getattr(prov.bbox, 't', 0), 
                                            "r": getattr(prov.bbox, 'r', 0),
                                            "b": getattr(prov.bbox, 'b', 0),
                                            "coord_origin": getattr(prov.bbox, 'coord_origin', 'BOTTOMLEFT')
                                        }
                                    
                                    # charspan ì •ë³´ ì¶”ê°€
                                    if hasattr(prov, 'charspan'):
                                        bbox_info["charspan"] = prov.charspan
                                    
                                    bbox_data.append(bbox_info)
                                
                                # Excel ì‹œíŠ¸ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                                if hasattr(prov, 'sheet_name'):
                                    sheet_names.add(prov.sheet_name)
                                elif hasattr(prov, 'page_name'):
                                    sheet_names.add(prov.page_name)
                
                # í˜ì´ì§€ ì •ë³´ ì„¤ì •
                # íŒŒì¼ íƒ€ì…ë³„ë¡œ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
                if file_extension == '.xlsx':
                    # Excel: page_noëŠ” ì‹œíŠ¸ ìˆœì„œ, ë³„ë„ë¡œ sheet í•„ë“œ ì‚¬ìš©
                    chunk_info["page_info"] = []
                    chunk_info["pages"] = []  # Excelì€ í˜ì´ì§€ ì—†ìŒ
                    
                    # ì‹œíŠ¸ ì •ë³´ ì¶”ê°€
                    if sheet_names:
                        chunk_info["sheet_names"] = sorted(list(sheet_names))
                        chunk_info["sheet"] = list(sheet_names)[0] if len(sheet_names) == 1 else sorted(list(sheet_names))
                    elif excel_sheets and page_numbers:
                        # page_noë¡œ ì‹œíŠ¸ ë§¤ì¹­ (page_no 1 = ì²«ë²ˆì§¸ ì‹œíŠ¸)
                        matched_sheets = []
                        for page_no in sorted(page_numbers):
                            if page_no <= len(excel_sheets):
                                matched_sheets.append(excel_sheets[page_no - 1])
                        if matched_sheets:
                            chunk_info["sheet_names"] = matched_sheets
                            chunk_info["sheet"] = matched_sheets[0] if len(matched_sheets) == 1 else matched_sheets
                            chunk_info["sheet_index"] = sorted(list(page_numbers))  # ì‹œíŠ¸ ìˆœì„œ
                    
                elif page_numbers:
                    # PDF ë“±: í˜ì´ì§€ ì •ë³´
                    chunk_info["page_info"] = sorted(list(page_numbers))
                    chunk_info["pages"] = sorted(list(page_numbers))
                else:
                    # DOCX ë“±: ì„¹ì…˜ ì •ë³´
                    chunk_info["page_info"] = []
                    chunk_info["pages"] = []
                    chunk_info["section_index"] = i + 1  # ì²­í¬ ìˆœì„œ
                    
                chunk_info["bbox_info"] = bbox_data
                
                chunk_data.append(chunk_info)
            
            # JSON ì‘ë‹µ ì¤€ë¹„
            response_data = {
                "success": True,
                "filename": file.filename,
                "file_type": SUPPORTED_EXTENSIONS.get(file_extension, "ì•Œ ìˆ˜ ì—†ìŒ"),
                "total_chunks": len(chunks),
                "chunking_config": {
                    "tokenizer": EMBED_MODEL_ID,
                    "max_tokens": max_tokens or MAX_TOKENS,
                    "merge_peers": True,
                    "contextualize": contextualize
                },
                "chunks": chunk_data
            }
            
            if include_metadata:
                # ë¬¸ì„œ ì „ì²´ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
                total_pages = 0
                if hasattr(doc, 'pages') and doc.pages:
                    total_pages = len(doc.pages)
                elif hasattr(doc, 'main_text') and hasattr(doc.main_text, 'page'):
                    # ë¬¸ì„œì˜ ëª¨ë“  ì•„ì´í…œì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
                    max_page = 0
                    for item in doc.main_text:
                        if hasattr(item, 'prov') and hasattr(item.prov, 'page'):
                            max_page = max(max_page, item.prov.page)
                    total_pages = max_page
                
                response_data["file_metadata"] = {
                    "original_file_size": len(content),
                    "mime_type": mimetypes.guess_type(file.filename)[0],
                    "extension": file_extension,
                    "total_pages": total_pages if total_pages > 0 else None
                }
            
            return response_data
            
        except Exception as e:
            logger.error(f"ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "filename": file.filename
            }

# ==================== ë¹„ë™ê¸° ì‘ì—… ì—”ë“œí¬ì¸íŠ¸ (n8n íƒ€ì„ì•„ì›ƒ ë°©ì§€) ====================

def process_chunking_job(
    job_id: str,
    file_path: str,
    filename: str,
    file_extension: str,
    include_metadata: bool,
    contextualize: bool,
    max_tokens: Optional[int]
):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²­í‚¹ ì‘ì—… ì²˜ë¦¬ (ë™ê¸° í•¨ìˆ˜ë¡œ ì‹¤í–‰ë¨)"""
    logger.info(f"ğŸš€ [Job {job_id}] ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ë¨! íŒŒì¼: {filename}")
    try:
        job_store[job_id]["status"] = "processing"
        job_store[job_id]["progress"] = 10
        logger.info(f"[Job {job_id}] ì²­í‚¹ ì‘ì—… ì‹œì‘: {filename}")
        
        # ë¬¸ì„œ ë³€í™˜
        job_store[job_id]["progress"] = 30
        job_store[job_id]["message"] = "ë¬¸ì„œ ë³€í™˜ ì¤‘..."
        result = doc_converter.convert(file_path)
        doc = result.document
        logger.info(f"[Job {job_id}] ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ")
        
        # ì‚¬ìš©ì ì •ì˜ í† í° ìˆ˜ê°€ ìˆë‹¤ë©´ ìƒˆë¡œìš´ chunker ìƒì„±
        job_store[job_id]["progress"] = 40
        if max_tokens and max_tokens != MAX_TOKENS:
            custom_tokenizer = HuggingFaceTokenizer(
                tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),
                max_tokens=max_tokens,
            )
            custom_chunker = HybridChunker(
                tokenizer=custom_tokenizer,
                merge_peers=True,
            )
            active_chunker = custom_chunker
        else:
            active_chunker = chunker
        
        # Excel íŒŒì¼ì¸ ê²½ìš° ì‹œíŠ¸ ì •ë³´ ì§ì ‘ ì¶”ì¶œ
        excel_sheets = []
        if file_extension == '.xlsx':
            try:
                import openpyxl
                wb = openpyxl.load_workbook(file_path, read_only=True)
                excel_sheets = wb.sheetnames
                wb.close()
                logger.info(f"[Job {job_id}] Excel ì‹œíŠ¸ ê°ì§€: {excel_sheets}")
            except Exception as e:
                logger.warning(f"[Job {job_id}] Excel ì‹œíŠ¸ ì´ë¦„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # HybridChunkerë¡œ ì²­í‚¹
        job_store[job_id]["progress"] = 60
        job_store[job_id]["message"] = "ì²­í‚¹ ì¤‘..."
        chunks = list(active_chunker.chunk(dl_doc=doc))
        logger.info(f"[Job {job_id}] ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        # ì²­í¬ ì •ë³´ ìˆ˜ì§‘
        job_store[job_id]["progress"] = 80
        job_store[job_id]["message"] = "ì²­í¬ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ì¤‘..."
        chunk_data = []
        for i, chunk in enumerate(chunks):
            chunk_info = {
                "chunk_id": i + 1,
                "text": chunk.text,
                "text_length": len(chunk.text),
                "token_count": tokenizer.count_tokens(chunk.text) if hasattr(tokenizer, 'count_tokens') else None
            }
            
            # ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©
            if contextualize:
                enriched_text = active_chunker.contextualize(chunk=chunk)
                chunk_info["contextualized_text"] = enriched_text
                chunk_info["contextualized_length"] = len(enriched_text)
            
            # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (ì™„ì „í•œ ë²„ì „ - í˜ì´ì§€ ë° ì‹œíŠ¸ ì •ë³´ í¬í•¨)
            if hasattr(chunk, 'meta'):
                chunk_info["metadata"] = chunk.meta.export_json_dict() if hasattr(chunk.meta, 'export_json_dict') else str(chunk.meta)
            
            # í˜ì´ì§€ ë° ì‹œíŠ¸ ì •ë³´ ì¶”ì¶œ
            page_numbers = set()
            bbox_data = []
            sheet_names = set()
            
            if hasattr(chunk, 'meta') and hasattr(chunk.meta, 'doc_items'):
                for doc_item in chunk.meta.doc_items:
                    if hasattr(doc_item, 'prov') and isinstance(doc_item.prov, list):
                        for prov in doc_item.prov:
                            # PDF í˜ì´ì§€ ì •ë³´
                            if hasattr(prov, 'page_no') and prov.page_no > 0:
                                page_numbers.add(prov.page_no)
                                
                                bbox_info = {
                                    "page": prov.page_no,
                                    "label": getattr(doc_item, 'label', 'unknown')
                                }
                                
                                # bbox ì •ë³´ ì¶”ê°€
                                if hasattr(prov, 'bbox'):
                                    bbox_info["bbox"] = {
                                        "l": getattr(prov.bbox, 'l', 0),
                                        "t": getattr(prov.bbox, 't', 0),
                                        "r": getattr(prov.bbox, 'r', 0),
                                        "b": getattr(prov.bbox, 'b', 0),
                                        "coord_origin": getattr(prov.bbox, 'coord_origin', 'BOTTOMLEFT')
                                    }
                                
                                # charspan ì •ë³´ ì¶”ê°€
                                if hasattr(prov, 'charspan'):
                                    bbox_info["charspan"] = prov.charspan
                                
                                bbox_data.append(bbox_info)
                            
                            # Excel ì‹œíŠ¸ ì •ë³´
                            if hasattr(prov, 'sheet_name'):
                                sheet_names.add(prov.sheet_name)
                            elif hasattr(prov, 'page_name'):
                                sheet_names.add(prov.page_name)
            
            # íŒŒì¼ íƒ€ì…ë³„ ì •ë³´ ì¶”ê°€
            if file_extension == '.xlsx':
                # Excel: page_noëŠ” ì‹œíŠ¸ ìˆœì„œ, ë³„ë„ë¡œ sheet í•„ë“œ ì‚¬ìš©
                chunk_info["page_info"] = []
                chunk_info["pages"] = []  # Excelì€ í˜ì´ì§€ ì—†ìŒ
                
                # ì‹œíŠ¸ ì •ë³´ ì¶”ê°€
                if sheet_names:
                    chunk_info["sheet_names"] = sorted(list(sheet_names))
                    chunk_info["sheet"] = list(sheet_names)[0] if len(sheet_names) == 1 else sorted(list(sheet_names))
                elif excel_sheets and page_numbers:
                    # page_noë¡œ ì‹œíŠ¸ ë§¤ì¹­ (page_no 1 = ì²«ë²ˆì§¸ ì‹œíŠ¸)
                    matched_sheets = []
                    for page_no in sorted(page_numbers):
                        if page_no <= len(excel_sheets):
                            matched_sheets.append(excel_sheets[page_no - 1])
                    if matched_sheets:
                        chunk_info["sheet_names"] = matched_sheets
                        chunk_info["sheet"] = matched_sheets[0] if len(matched_sheets) == 1 else matched_sheets
                        chunk_info["sheet_index"] = sorted(list(page_numbers))  # ì‹œíŠ¸ ìˆœì„œ
                
            elif page_numbers:
                # PDF ë“±: í˜ì´ì§€ ì •ë³´
                chunk_info["page_info"] = sorted(list(page_numbers))
                chunk_info["pages"] = sorted(list(page_numbers))
            else:
                # DOCX ë“±: ì„¹ì…˜ ì •ë³´
                chunk_info["page_info"] = []
                chunk_info["pages"] = []
                chunk_info["section_index"] = i + 1  # ì²­í¬ ìˆœì„œ
            
            # bbox ì •ë³´ ì¶”ê°€
            chunk_info["bbox_info"] = bbox_data
            
            chunk_data.append(chunk_info)
        
        # Excel ì‹œíŠ¸ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€
        result_data = {
            "success": True,
            "filename": filename,
            "file_type": SUPPORTED_EXTENSIONS.get(file_extension, "ì•Œ ìˆ˜ ì—†ìŒ"),
            "total_chunks": len(chunks),
            "chunking_config": {
                "tokenizer": EMBED_MODEL_ID,
                "max_tokens": max_tokens or MAX_TOKENS,
                "merge_peers": True,
                "contextualize": contextualize
            },
            "chunks": chunk_data
        }
        
        # Excel íŒŒì¼ì¸ ê²½ìš° ì „ì²´ ì‹œíŠ¸ ëª©ë¡ ì¶”ê°€
        if file_extension == '.xlsx' and excel_sheets:
            result_data["excel_sheets"] = excel_sheets
            result_data["total_sheets"] = len(excel_sheets)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        job_store[job_id]["progress"] = 100
        job_store[job_id]["status"] = "completed"
        job_store[job_id]["message"] = "ì²˜ë¦¬ ì™„ë£Œ"
        job_store[job_id]["result"] = result_data
        job_store[job_id]["completed_at"] = datetime.now().isoformat()
        logger.info(f"âœ… [Job {job_id}] ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ [Job {job_id}] ì‘ì—… ì‹¤íŒ¨: {str(e)}", exc_info=True)
        job_store[job_id]["status"] = "failed"
        job_store[job_id]["progress"] = 0
        job_store[job_id]["error"] = str(e)
        job_store[job_id]["failed_at"] = datetime.now().isoformat()
    
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_dir = job_store[job_id].get("temp_dir")
        if temp_dir and os.path.exists(temp_dir):
            try:
                import shutil
                shutil.rmtree(temp_dir)
                logger.info(f"ğŸ—‘ï¸ [Job {job_id}] ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {temp_dir}")
            except Exception as cleanup_error:
                logger.warning(f"âš ï¸ [Job {job_id}] ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_error}")

@app.post("/convert-chunked-async")
async def convert_file_chunked_async(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="ë³€í™˜í•  íŒŒì¼"),
    include_metadata: bool = Query(False, description="ì²­í¬ ë©”íƒ€ë°ì´í„° í¬í•¨"),
    contextualize: bool = Query(True, description="ì²­í¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©"),
    max_tokens: Optional[int] = Query(None, description="ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 512)")
):
    """
    ë¹„ë™ê¸° ì²­í‚¹ ì‘ì—…ì„ ì‹œì‘í•˜ê³  ì¦‰ì‹œ job_idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ íƒ€ì„ì•„ì›ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    ë°˜í™˜ëœ job_idë¡œ /job/{job_id} ì—”ë“œí¬ì¸íŠ¸ë¥¼ pollingí•˜ì—¬ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    logger.info(f"ğŸ“¥ [API í˜¸ì¶œ] POST /convert-chunked-async - íŒŒì¼: {file.filename}")
    
    file_extension = Path(file.filename).suffix.lower()
    if file_extension not in SUPPORTED_EXTENSIONS:
        supported_list = ', '.join(SUPPORTED_EXTENSIONS.keys())
        logger.warning(f"âŒ [API í˜¸ì¶œ] ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
        raise HTTPException(
            status_code=400, 
            detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {supported_list}"
        )
    
    # Job ID ìƒì„±
    job_id = str(uuid.uuid4())
    logger.info(f"ğŸ†” [API í˜¸ì¶œ] ìƒì„±ëœ Job ID: {job_id}")
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_dir = tempfile.mkdtemp()
    file_path = os.path.join(temp_dir, file.filename)
    
    try:
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Job ìƒíƒœ ì´ˆê¸°í™”
        job_store[job_id] = {
            "job_id": job_id,
            "status": "queued",
            "progress": 0,
            "message": "ì‘ì—… ëŒ€ê¸° ì¤‘...",
            "filename": file.filename,
            "file_size": len(content),
            "created_at": datetime.now().isoformat(),
            "temp_dir": temp_dir
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        background_tasks.add_task(
            process_chunking_job,
            job_id=job_id,
            file_path=file_path,
            filename=file.filename,
            file_extension=file_extension,
            include_metadata=include_metadata,
            contextualize=contextualize,
            max_tokens=max_tokens
        )
        
        logger.info(f"[Job {job_id}] ë¹„ë™ê¸° ì‘ì—… ìƒì„±: {file.filename} ({len(content)} bytes)")
        
        return {
            "job_id": job_id,
            "status": "queued",
            "message": "ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. /job/{job_id} ì—”ë“œí¬ì¸íŠ¸ë¡œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "filename": file.filename,
            "file_size": len(content),
            "created_at": job_store[job_id]["created_at"]
        }
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if os.path.exists(temp_dir):
            import shutil
            shutil.rmtree(temp_dir)
        logger.error(f"ì‘ì—… ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ì—… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.get("/job/{job_id}")
async def get_job_status(job_id: str):
    """
    ì‘ì—… ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    n8nì—ì„œ ì´ ì—”ë“œí¬ì¸íŠ¸ë¥¼ pollingí•˜ì—¬ ì‘ì—… ì™„ë£Œ ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” [API í˜¸ì¶œ] GET /job/{job_id} - ìƒíƒœ ì¡°íšŒ ìš”ì²­ ë°›ìŒ")
    
    if job_id not in job_store:
        logger.warning(f"âŒ [API í˜¸ì¶œ] GET /job/{job_id} - ì‘ì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        raise HTTPException(status_code=404, detail=f"ì‘ì—… ID '{job_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì½ê¸° (deep copy)
    import copy
    job = copy.deepcopy(job_store[job_id])
    
    logger.info(f"ğŸ“Š [API í˜¸ì¶œ] GET /job/{job_id} - í˜„ì¬ ìƒíƒœ: {job['status']}, ì§„í–‰ë¥ : {job.get('progress', 0)}%")
    
    response = {
        "job_id": job_id,
        "status": job["status"],
        "progress": job.get("progress", 0),
        "message": job.get("message", ""),
        "filename": job.get("filename"),
        "created_at": job.get("created_at")
    }
    
    if job["status"] == "completed":
        response["completed_at"] = job.get("completed_at")
        response["result"] = job.get("result")
        logger.info(f"âœ… [API í˜¸ì¶œ] GET /job/{job_id} - ì‘ì—… ì™„ë£Œ ì‘ë‹µ ë°˜í™˜")
    elif job["status"] == "failed":
        response["failed_at"] = job.get("failed_at")
        response["error"] = job.get("error")
        logger.error(f"âŒ [API í˜¸ì¶œ] GET /job/{job_id} - ì‘ì—… ì‹¤íŒ¨ ì‘ë‹µ ë°˜í™˜: {job.get('error')}")
    else:
        logger.info(f"â³ [API í˜¸ì¶œ] GET /job/{job_id} - ì§„í–‰ ì¤‘ ì‘ë‹µ ë°˜í™˜")
    
    return response

@app.delete("/job/{job_id}")
async def delete_job(job_id: str):
    """
    ì™„ë£Œëœ ì‘ì—…ì„ ì‚­ì œí•˜ê³  ì„ì‹œ íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    if job_id not in job_store:
        raise HTTPException(status_code=404, detail=f"ì‘ì—… ID '{job_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    job = job_store[job_id]
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if "temp_dir" in job and os.path.exists(job["temp_dir"]):
        import shutil
        shutil.rmtree(job["temp_dir"])
        logger.info(f"[Job {job_id}] ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    
    # Job ì‚­ì œ
    del job_store[job_id]
    logger.info(f"[Job {job_id}] ì‘ì—… ì‚­ì œ ì™„ë£Œ")
    
    return {
        "message": f"ì‘ì—… {job_id}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "job_id": job_id
    }

@app.get("/jobs")
async def list_jobs():
    """
    ëª¨ë“  ì‘ì—… ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    jobs = []
    for job_id, job in job_store.items():
        jobs.append({
            "job_id": job_id,
            "status": job["status"],
            "progress": job.get("progress", 0),
            "filename": job.get("filename"),
            "created_at": job.get("created_at")
        })
    
    return {
        "total_jobs": len(jobs),
        "jobs": jobs
    }

# ==================== ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/convert-multiple")
async def convert_multiple_files(
    files: List[UploadFile] = File(..., description="ë³€í™˜í•  íŒŒì¼ë“¤"),
    output_format: Literal["zip", "json"] = Query("zip", description="ì¶œë ¥ í˜•ì‹"),
    use_chunking: bool = Query(False, description="HybridChunkerë¥¼ ì‚¬ìš©í•œ ì²­í‚¹ ì ìš©"),
    contextualize: bool = Query(True, description="ì²­í¬ ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì ìš©")
):
    """
    ì—¬ëŸ¬ íŒŒì¼ì„ ì¼ê´„ ë³€í™˜í•©ë‹ˆë‹¤.
    
    - zip: ê° íŒŒì¼ì„ Markdownìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ZIP íŒŒì¼ë¡œ ë°˜í™˜
    - json: ëª¨ë“  ë³€í™˜ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜
    """
    
    if len(files) > 10:
        raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 10ê°œ íŒŒì¼ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    results = []
    converted_files = {}
    
    with tempfile.TemporaryDirectory() as temp_dir:
        for file in files:
            try:
                file_extension = Path(file.filename).suffix.lower()
                
                # ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ì€ ìŠ¤í‚µ
                if file_extension not in SUPPORTED_EXTENSIONS:
                    results.append({
                        "filename": file.filename,
                        "success": False,
                        "error": "ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹"
                    })
                    continue
                
                # íŒŒì¼ ì €ì¥
                input_file_path = os.path.join(temp_dir, file.filename)
                with open(input_file_path, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)
                
                # Docling ë³€í™˜
                result = doc_converter.convert(input_file_path)
                doc = result.document
                
                # ì²­í‚¹ ì ìš© ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬
                if use_chunking:
                    chunks = list(chunker.chunk(dl_doc=doc))
                    markdown_parts = []
                    for i, chunk in enumerate(chunks):
                        if contextualize:
                            enriched_text = chunker.contextualize(chunk=chunk)
                            markdown_parts.append(f"## ì²­í¬ {i+1}\n\n{enriched_text}\n")
                        else:
                            markdown_parts.append(f"## ì²­í¬ {i+1}\n\n{chunk.text}\n")
                    final_content = "\n".join(markdown_parts)
                else:
                    final_content = doc.export_to_markdown()
                
                # ê²°ê³¼ ì €ì¥
                base_name = Path(file.filename).stem
                converted_files[f"{base_name}.md"] = final_content
                
                results.append({
                    "filename": file.filename,
                    "success": True,
                    "output_filename": f"{base_name}.md",
                    "content_length": len(final_content),
                    "chunks_count": len(list(chunker.chunk(dl_doc=doc))) if use_chunking else 1
                })
                
                logger.info(f"ë³€í™˜ ì™„ë£Œ: {file.filename}")
                
            except Exception as e:
                results.append({
                    "filename": file.filename,
                    "success": False,
                    "error": str(e)
                })
                logger.error(f"íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨ ({file.filename}): {str(e)}")
    
    # JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
    if output_format == "json":
        return {
            "total_files": len(files),
            "successful_conversions": len(converted_files),
            "chunking_applied": use_chunking,
            "contextualize_applied": contextualize,
            "results": results,
            "converted_content": {
                filename: content 
                for filename, content in converted_files.items()
            }
        }
    
    # ZIP í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
    if converted_files:
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for filename, content in converted_files.items():
                zip_file.writestr(filename, content)
        
        zip_buffer.seek(0)
        
        return Response(
            content=zip_buffer.getvalue(),
            media_type="application/zip",
            headers={"Content-Disposition": "attachment; filename=docling_converted_files.zip"}
        )
    else:
        raise HTTPException(status_code=400, detail="ë³€í™˜ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

@app.get("/supported-formats")
async def get_supported_formats():
    """ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ ëª©ë¡ ë°˜í™˜"""
    return {
        "supported_extensions": SUPPORTED_EXTENSIONS,
        "total_formats": len(SUPPORTED_EXTENSIONS),
        "categories": {
            "documents": [".pdf", ".docx", ".pptx", ".xlsx"],
            "web": [".html", ".htm", ".md", ".csv", ".json", ".xml"],
            "images": [".jpg", ".jpeg", ".png", ".gif", ".bmp"],
            "text": [".txt"]
        },
        "chunking_info": {
            "engine": "HybridChunker",
            "tokenizer": EMBED_MODEL_ID,
            "default_max_tokens": MAX_TOKENS,
            "features": ["contextualize", "merge_peers", "hierarchical_chunking"]
        }
    }

@app.get("/chunking-info")
async def get_chunking_info():
    """HybridChunker ì„¤ì • ì •ë³´ ë°˜í™˜"""
    return {
        "chunker_type": "HybridChunker",
        "tokenizer": {
            "model": EMBED_MODEL_ID,
            "max_tokens": MAX_TOKENS,
            "type": "HuggingFaceTokenizer"
        },
        "features": {
            "contextualize": "ì²­í¬ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ê°•í™”",
            "merge_peers": "ì¸ì ‘í•œ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ë³‘í•©",
            "hierarchical_chunking": "ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ê³„ì¸µì  ì²­í‚¹"
        },
        "supported_operations": [
            "chunk(dl_doc): ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• ",
            "contextualize(chunk): ì²­í¬ì˜ ì»¨í…ìŠ¤íŠ¸ ê°•í™”"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("=" * 80)
    logger.info("ğŸš€ Docling RAG ì„œë²„ ì‹œì‘ ì¤‘...")
    logger.info(f"ğŸ“ ì—”ë“œí¬ì¸íŠ¸: http://0.0.0.0:10002")
    logger.info(f"ğŸ“š ë¬¸ì„œ: http://0.0.0.0:10002/docs")
    logger.info(f"ğŸ”§ ë¹„ë™ê¸° API: POST /convert-chunked-async")
    logger.info(f"ğŸ” ìƒíƒœ ì¡°íšŒ: GET /job/{{job_id}}")
    logger.info("=" * 80)
    uvicorn.run(app, host="0.0.0.0", port=10002, log_level="info")